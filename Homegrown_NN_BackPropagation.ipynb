{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_BackPropagation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "I9ggyaRjcO25",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from random import *\n",
        "def sigmoid(x):\n",
        "    return 1.0/(1.0 + np.exp(-x))\n",
        "def sigmoid_prime(x):\n",
        "    return sigmoid(x)*(1.0-sigmoid(x))\n",
        "  \n",
        "def compute_cost(A, Y):\n",
        "    \"\"\"\n",
        "    \n",
        "    Arguments:\n",
        "    A -- The sigmoid output of the activation, of shape (1, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    cost -- cross-entropy cost \n",
        "    \"\"\"\n",
        "\n",
        "    # Compute the cross-entropy cost\n",
        "#     if A[0] == 1:\n",
        "#       A[0] = 0.99\n",
        "    cost = np.multiply(np.log(A), Y) + np.multiply((1 - Y), np.log(1 - A))\n",
        "    cost = -1 * cost\n",
        "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
        "                                # E.g., turns [[17]] into 17 \n",
        "    return cost\n",
        "\n",
        "def CrossEntropy(yhat, y):\n",
        "    epsilon=0.000001\n",
        "#     if y == 1:\n",
        "#       return -np.log(yHat)+dummy\n",
        "#     else:\n",
        "#       return np.log(1 - yHat)\n",
        "    return -1*((y) * np.log(epsilon + yhat)) + ((1 - y) * np.log(1- (epsilon + yhat)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yXU4KH8icZ_0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "\n",
        "    def __init__(self, layers, activation='sigmoid'):\n",
        "        if activation == 'sigmoid':\n",
        "            self.activation = sigmoid\n",
        "            self.activation_prime = sigmoid_prime\n",
        "            self.cost = compute_cost\n",
        "            self.ce = CrossEntropy\n",
        "        self.weights = []\n",
        "#         layers = [2,2,1]\n",
        "#         range of weight values (-1,1)\n",
        "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
        "        for i in range(1, len(layers) - 1):\n",
        "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
        "            self.weights.append(r)\n",
        "        # output layer - random((2+1, 1)) : 3 x 1\n",
        "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
        "        self.weights.append(r)\n",
        "        \n",
        "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
        "          # Add column of ones to X\n",
        "          # This is to add the bias unit to the input layer\n",
        "          ones = np.atleast_2d(np.ones(X.shape[0]))\n",
        "          X = np.concatenate((ones.T, X), axis=1)\n",
        "          arr = []\n",
        "\n",
        "          for k in range(epochs):\n",
        "              if k % 10000 == 0: print('epochs:', k)\n",
        "\n",
        "              i = np.random.randint(X.shape[0])\n",
        "              a = [X[i]]\n",
        "              arr=[X[i]]\n",
        "\n",
        "              for l in range(len(self.weights)):\n",
        "                  dot_value = np.dot(a[l], self.weights[l])\n",
        "                  arr.append(dot_value)\n",
        "                  activation = self.activation(dot_value)\n",
        "                  a.append(activation)\n",
        "              # output layer\n",
        "#               error = y[i] - a[-1]\n",
        "#               error = self.cost(a[-1], y[i])\n",
        "              error  = self.ce(a[-1], y[i])\n",
        "              deltas = [error * self.activation_prime(arr[-1])]\n",
        "\n",
        "              # we need to begin at the second to last layer \n",
        "              # (a layer before the output layer)\n",
        "              for l in range(len(a) - 2, 0, -1): \n",
        "                  deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(arr[l]))\n",
        "\n",
        "              # reverse\n",
        "              # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
        "              deltas.reverse()\n",
        "\n",
        "              # backpropagation\n",
        "              # 1. Multiply its output delta and input activation \n",
        "              #    to get the gradient of the weight.\n",
        "              # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
        "              for i in range(len(self.weights)):\n",
        "                  layer = np.atleast_2d(a[i])\n",
        "                  delta = np.atleast_2d(deltas[i])\n",
        "                  self.weights[i] += learning_rate * layer.T.dot(delta)\n",
        "                  \n",
        "#           print(np.max(arr))\n",
        "            \n",
        "    def predict(self, x): \n",
        "          a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
        "          for l in range(0, len(self.weights)):\n",
        "              a = self.activation(np.dot(a, self.weights[l]))\n",
        "          return a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_ZUxc3uUgK-3",
        "colab_type": "code",
        "outputId": "00db871f-aa8e-4266-bb2e-14d677d0c034",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "nn = NeuralNetwork([2,3,3,1])\n",
        "inp_class1=[[1,0],[-1,0]]\n",
        "inp_class2=[[0,1],[0,-1]]\n",
        "out_class1=[0,0]\n",
        "out_class2=[1,1]\n",
        "for i in range(0,48):\n",
        "  x=uniform(1,10)\n",
        "  y=uniform(-1,0)\n",
        "  a=uniform(-1,0)\n",
        "  b=uniform(1,10)\n",
        "  foo=[x,y]\n",
        "  foo1=[a,b]\n",
        "  inp_class1.append(foo)\n",
        "  out_class1.append(0)\n",
        "  inp_class2.append(foo1)\n",
        "  out_class2.append(1)\n",
        "inp=inp_class1+inp_class2\n",
        "out=out_class1+out_class2\n",
        "X = np.array(inp)\n",
        "y = np.array(out)\n",
        "nn.fit(X, y)\n",
        "for e in X:\n",
        "  print(e,nn.predict(e))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epochs: 0\n",
            "epochs: 10000\n",
            "epochs: 20000\n",
            "epochs: 30000\n",
            "epochs: 40000\n",
            "epochs: 50000\n",
            "epochs: 60000\n",
            "epochs: 70000\n",
            "epochs: 80000\n",
            "epochs: 90000\n",
            "[1. 0.] [0.02230407]\n",
            "[-1.  0.] [0.08369524]\n",
            "[ 7.93242792 -0.9378195 ] [0.00046126]\n",
            "[ 4.45758895 -0.24809318] [0.00065122]\n",
            "[ 6.62492415 -0.71596716] [0.0004878]\n",
            "[ 7.9935218  -0.50676232] [0.00063068]\n",
            "[ 5.01664373 -0.5425838 ] [0.00050667]\n",
            "[ 6.81921075 -0.20534064] [0.00095082]\n",
            "[ 7.78477386 -0.95804342] [0.00045693]\n",
            "[ 1.36050221 -0.53168759] [0.00115423]\n",
            "[ 7.04952968 -0.31366535] [0.0007767]\n",
            "[ 5.04671612 -0.33105528] [0.00061464]\n",
            "[ 5.95964547 -0.4119798 ] [0.00059992]\n",
            "[ 9.51838659 -0.75068987] [0.00054186]\n",
            "[ 9.79417624 -0.17128901] [0.00200661]\n",
            "[ 6.35619517 -0.87391765] [0.00045547]\n",
            "[ 8.8812654  -0.77591691] [0.00051462]\n",
            "[ 9.49003416 -0.36289416] [0.00099991]\n",
            "[ 2.16469494 -0.62157176] [0.00046892]\n",
            "[ 6.19849214 -0.55569211] [0.00053062]\n",
            "[ 3.35134256 -0.10925252] [0.00072083]\n",
            "[ 5.10857172 -0.29858082] [0.00064471]\n",
            "[ 3.02768472 -0.1191948 ] [0.00068538]\n",
            "[ 7.8476441  -0.18153697] [0.00122496]\n",
            "[ 3.7448579  -0.47985944] [0.00049996]\n",
            "[ 7.81595004 -0.29892156] [0.00089131]\n",
            "[ 7.2669714  -0.73085196] [0.00049512]\n",
            "[ 6.22660992 -0.68037253] [0.00049012]\n",
            "[ 9.24031611 -0.3684577 ] [0.00094586]\n",
            "[ 3.42723863 -0.37237776] [0.00053033]\n",
            "[ 5.76172185 -0.90281481] [0.0004475]\n",
            "[ 1.34775555 -0.64204712] [0.00120238]\n",
            "[ 4.52238074 -0.38992687] [0.00055576]\n",
            "[ 5.05253782 -0.02173266] [0.00113388]\n",
            "[ 4.49299513 -0.01447497] [0.00104331]\n",
            "[ 2.3235974  -0.34938064] [0.00052089]\n",
            "[ 6.49135046 -0.59224325] [0.00052377]\n",
            "[ 7.60877013 -0.51948627] [0.00060142]\n",
            "[ 5.94437455 -0.90003253] [0.00044907]\n",
            "[ 2.40788491 -0.78081978] [0.0004449]\n",
            "[ 5.37472185 -0.1289968 ] [0.00090709]\n",
            "[ 5.21233723 -0.97723608] [0.00043804]\n",
            "[ 6.79134532 -0.44173972] [0.00061872]\n",
            "[ 9.02303724 -0.99976643] [0.0004629]\n",
            "[ 3.95851425 -0.10612766] [0.00077973]\n",
            "[ 9.31495912 -0.08753073] [0.00249037]\n",
            "[ 5.69217647 -0.29196007] [0.00068938]\n",
            "[ 8.41648889 -0.38244363] [0.00081133]\n",
            "[ 3.20374071 -0.99295673] [0.00043014]\n",
            "[ 3.70070214 -0.5662233 ] [0.00047754]\n",
            "[0. 1.] [0.99363307]\n",
            "[ 0. -1.] [0.92746666]\n",
            "[-0.36461639  5.15051697] [0.99555057]\n",
            "[-0.18054761  8.96064439] [0.99438095]\n",
            "[-0.63124755  2.8961926 ] [0.99415215]\n",
            "[-0.48563363  7.0542074 ] [0.99564155]\n",
            "[-0.18484655  8.95083384] [0.99442124]\n",
            "[-0.64863322  6.8091106 ] [0.99561996]\n",
            "[-0.9599735   2.18854283] [0.98081572]\n",
            "[-0.81844108  1.01952718] [0.95189753]\n",
            "[-0.01812233  9.61827114] [0.99075836]\n",
            "[-0.36569111  1.10245444] [0.99247051]\n",
            "[-0.58174911  3.07081552] [0.99460525]\n",
            "[-0.62527745  6.85367097] [0.99563554]\n",
            "[-0.95332762  7.07139423] [0.99521574]\n",
            "[-0.9241356   3.52721216] [0.99126698]\n",
            "[-0.12473031  3.3237476 ] [0.99536939]\n",
            "[-0.20200745  1.98331891] [0.99518235]\n",
            "[-0.34560767  4.07732412] [0.99546869]\n",
            "[-0.96509903  4.86305004] [0.99345077]\n",
            "[-0.0075476   3.97036083] [0.99508072]\n",
            "[-0.82945052  4.88680463] [0.99456867]\n",
            "[-0.05342106  3.45802029] [0.99526214]\n",
            "[-0.92336993  9.80894243] [0.99575226]\n",
            "[-0.62195999  2.7311278 ] [0.99405254]\n",
            "[-0.32136474  4.55120485] [0.99551199]\n",
            "[-0.11111349  5.99669819] [0.99504796]\n",
            "[-0.72724703  6.12847913] [0.99542838]\n",
            "[-0.64041059  3.86194809] [0.99482812]\n",
            "[-0.49488971  2.92847889] [0.99489711]\n",
            "[-0.83150424  2.72195118] [0.99094943]\n",
            "[-0.21398796  4.33773222] [0.99545294]\n",
            "[-0.69174915  7.52081732] [0.99566661]\n",
            "[-0.18937608  9.7326829 ] [0.99399982]\n",
            "[-0.56840839  6.23813468] [0.9956022]\n",
            "[-0.41573833  6.73564693] [0.99559943]\n",
            "[-0.058234    7.00788295] [0.99438403]\n",
            "[-0.00773392  6.18774456] [0.99438729]\n",
            "[-0.66450929  3.95482405] [0.99477786]\n",
            "[-0.36753088  1.76807171] [0.99461897]\n",
            "[-0.72044217  1.17290949] [0.98099952]\n",
            "[-0.13610627  9.09130501] [0.99386198]\n",
            "[-0.08682663  6.75166544] [0.99469883]\n",
            "[-0.5197889   5.31819211] [0.99552416]\n",
            "[-0.16327273  2.49206643] [0.99533143]\n",
            "[-0.21116243  4.99998591] [0.99543333]\n",
            "[-0.01173266  2.09996199] [0.99521042]\n",
            "[-0.37044033  8.13756237] [0.99545141]\n",
            "[-0.40948948  6.50246239] [0.99559695]\n",
            "[-0.6479516   3.15217174] [0.99427702]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Blj_3_AvwYUX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}